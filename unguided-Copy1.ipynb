{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from multilabel import multilabel_sample_dataframe, multilabel_train_test_split\n",
    "from SparseInteractions import SparseInteractions\n",
    "from metrics import multi_multi_log_loss\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load data and Explore Data\n",
    "\n",
    "df = pd.read_csv(\"TrainingData.csv\", index_col = 0)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work on massive number of labels and resample\n",
    "labels = ['Function',\n",
    "          'Object_Type',\n",
    "          'Operating_Status',\n",
    "          'Position_Type',\n",
    "          'Pre_K',\n",
    "          'Reporting',\n",
    "          'Sharing',\n",
    "          'Student_Type',\n",
    "          'Use']\n",
    "non_labels = [c for c in df.columns if c not in labels]\n",
    "\n",
    "sample_size = 40000\n",
    "sampling = multilabel_sample_dataframe(df,\n",
    "  pd.get_dummies(df[labels]),\n",
    "  size = sample_size,\n",
    "  min_count=25,\n",
    "  seed=42)\n",
    "\n",
    "dummy_labels = pd.get_dummies(sampling[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test and split the data\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(\n",
    "    sampling[non_labels], \n",
    "    dummy_labels, \n",
    "    0.2, \n",
    "    min_count = 3, \n",
    "    seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the data\n",
    "NUMERIC_COLUMNS = ['FTE', 'Total']\n",
    "\n",
    "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + labels):\n",
    "    \"\"\" Takes the dataset as read in, drops the non-feature, non-text columns and\n",
    "        then combines all of the text columns into a single vector that has all of\n",
    "        the text for a row.\n",
    "        \n",
    "        :param data_frame: The data as read in with read_csv (no preprocessing necessary)\n",
    "        :param to_drop (optional): Removes the numeric and label columns by default.\n",
    "    \"\"\"\n",
    "    # drop non-text columns that are in the df\n",
    "    to_drop = set(['FTE', 'Total']) & set(data_frame.columns.tolist())\n",
    "    text_data = data_frame.drop(to_drop, axis=1)\n",
    "    \n",
    "    # replace nans with blanks\n",
    "    text_data.fillna(\"\", inplace=True)\n",
    "    \n",
    "    # joins all of the text items in a row (axis=1)\n",
    "    # with a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)\n",
    "\n",
    "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text_data.fit_transform(sampling.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_numeric_data.fit_transform(sampling.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss_scorer = make_scorer(multi_multi_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set a reasonable number of features before adding interactions\n",
    "chi_k = 300\n",
    "\n",
    "# create the pipeline object\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', SimpleImputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                     alternate_sign=False, norm=None, binary=False,\n",
    "                                                     ngram_range=(1, 2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# fit the pipeline to our training data\n",
    "pl.fit(X_train, y_train.values)\n",
    "\n",
    "# print the score of our trained pipeline on our test set\n",
    "print(\"Logloss score of trained pipeline: \", log_loss_scorer(pl, X_test, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load holdout data\n",
    "holdout = pd.read_csv('TestData.csv', index_col=0)\n",
    "\n",
    "# Make predictions\n",
    "predictions = pl.predict_proba(holdout)\n",
    "\n",
    "\n",
    "# Format correctly in new DataFrame: prediction_df\n",
    "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[labels]).columns,\n",
    "                             index=holdout.index,\n",
    "                             data=predictions)\n",
    "prediction_df.columns=prediction_df.columns.str.replace(\"_\", \"__\", 1)\n",
    "\n",
    "\n",
    "# Save prediction_df to csv called \"predictions.csv\"\n",
    "prediction_df.to_csv(\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
